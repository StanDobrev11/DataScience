get_ipython().run_line_magic("matplotlib", " inline")


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.decomposition import PCA


df_twitter = pd.read_csv('data/twitter_training.csv', header=None)


df_twitter.columns = ['tweet_id', 'entity', 'sentiment', 'content']


df_twitter.describe()


df_twitter.info()


df_twitter.tweet_id.nunique()


df_twitter.drop_duplicates()


df = df_twitter


with open('data/twitter_training.csv', 'r', encoding='utf-8') as file:
    print(file.read()[:300])




df.content.str.len().sort_values(ascending=False)


df.loc[70940, 'content']


df[df.content.isna()]


df = df.dropna()


df.shape


df.content.str.len().sort_values(ascending=False)


df[df.content.str.len() <= 3]





for sentiment, group_data in df.groupby('sentiment'):
    print(sentiment, group_data.content.str.len().median())


df.loc[0].content


Counter(df.loc[0].content)


all_text = ' '.join(df.content.ravel())


Counter(all_text)


Counter(all_text.lower())


frequencies = Counter(all_text.lower()).most_common(20)


frequencies


plt.bar([sym for sym, freq in frequencies], [freq for sym, freq in frequencies])
plt.ylabel('Count of Symbols')
plt.xlabel('Symbols')
plt.show()


words = all_text.split(' ')


Counter(words)


nltk.download('stopwords')


stopwords.words('english')


df['words'] = df.content.str.split('\\s+') # tokenization


df


df.words.apply(lambda x: len(x)).sort_values(ascending=False)


df[df.content.str.contains('happy')]


df[df.content.str.contains('happy')]


df[df.content.str.contains('happy', case=False)].sentiment.value_counts()


df[
    (df.content.str.contains('happy', case=False)) &
    (df.sentiment == 'Negative')
]


[w.replace('.', '').replace('!', '') for w in df.words.loc[42] if w != '']


def clean_words(words, symbols=['.', '!', '?'], replacement=''):
    for symbol in symbols:
        words = [w.replace(symbol, replacement) for w in words if w != '']
    return words


clean_words(df.words.loc[42])


df.words = df.words.apply(clean_words)


df.words = df.words.apply(lambda words: [w.lower() for w in words])


sample_line = df.loc[64970, 'words']


stemmer = PorterStemmer()


[stemmer.stem(w) for w in sample_line] 


sample_line


df['stemmed'] = df.words.apply(lambda list: [stemmer.stem(w) for w in list])


df.stemmed


sample_words = df.stemmed.sample(20)


sample_words.apply(Counter)


count_vectorizer = CountVectorizer()


count_vectorizer.fit(raw_documents=df.content)


count_vectorizer.vocabulary_


freq_table = count_vectorizer.transform(df.content)





model = MultinomialNB()


model.fit(freq_table, df.sentiment)


count_vectorizer2 = CountVectorizer(ngram_range=(1, 3), min_df=5)


count_vectorizer2.fit(df.content)


count_vectorizer2.transform(df.content)


tfidf = TfidfVectorizer()


tfidf.fit(df.content)


tfidf.vocabulary_


tfidf.transform(df.content)



