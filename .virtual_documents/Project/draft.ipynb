get_ipython().run_line_magic("matplotlib", " inline")


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import xarray as xr
import seaborn as sns
from scipy.interpolate import griddata
from plotting import plot_equatorial_pacific
from sst_utils import csv_from_aqua_modis_dataset, csv_from_avhrp_csv





data = pd.read_csv('data/Marine_CSV_sample.csv')


data.columns


data[['Latitude', 'Longitude', 'Time of Observation', 'Sea Surface Temperature']]





data = pd.read_csv('data/40_130_30_140.csv')


data.columns


data[['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SEA_SURF_TEMP']]


data.groupby(['DATE', 'LATITUDE', 'LONGITUDE'])['SEA_SURF_TEMP'].mean()


data['SEA_SURF_TEMP'][data['LATITUDE'] < 0]


data['LATITUDE'].max()





# reading the dataset for month of July 2024
# dataset = xr.open_dataset('data/aqua_modis.20240701_20240731.L3m.MO.SST.sst.9km.nc')


dataset = xr.open_dataset('data/aqua_modis/lanina/2010/AQUA_MODIS.20101201_20101231.L3m.MO.SST.sst.9km.nc')


# reading all variables
dataset.variables





# extracting to pandas dataframe
raw_data = dataset['sst'].to_dataframe()

# droping the multiindex
tidy_data = raw_data.reset_index()

# filtering lat[20S; 20N]
tidy_data = tidy_data[(tidy_data.lat >= -20) & (tidy_data.lat <= 20)]

# filtering long[130E -> 180 -> -80 W]
tidy_data = tidy_data[(tidy_data.lon >= -180) & (tidy_data.lon <= -80) | (tidy_data.lon >= 130) & (tidy_data.lon <= 180)]

# droping na values
tidy_data = tidy_data.dropna()

# rounding lats and long to 0.5 deg
def round_to_nearest_half(x):
    return np.round(x * 2) / 2

tidy_data.lat = tidy_data.lat.apply(round_to_nearest_half)
tidy_data.lon = tidy_data.lon.apply(round_to_nearest_half)

# grouping by lat and long and averaging the sst
tidy_data = tidy_data.groupby(['lat', 'lon']).sst.mean()

# dropping multiindex
tidy_data = tidy_data.reset_index()

# getting the month
current_month = dataset.attrs['time_coverage_start']

# convert the string to pd.Timestamp object
current_month = pd.to_datetime(current_month)

# normalize timestamp
current_month = current_month.normalize()

# set the month as index of the cleaned data
tidy_data.index = pd.Index([current_month] * len(tidy_data))


# rounding the sst
# raw_data.sst.round(1)
# since the .round() is not working as expected, np used
# np.round(raw_data.sst, 1)
# since np is not working, using apply
tidy_data.sst = tidy_data.sst.apply(lambda x: round(x, 1))


tidy_data['lon'] = np.where(tidy_data['lon'] < 0, tidy_data['lon'] + 360, tidy_data['lon'])


tidy_data.sort_values(['lat', 'lon'])


# getting both the sst and the quality of the observation
combined_raw = dataset[['sst', 'qual_sst']].to_dataframe()


combined_raw = combined_raw.reset_index()


combined_raw.qual_sst.value_counts(dropna=False)





dataset


tidy_data





plt.figure(figsize=(30, 10))

# contour map
lon_grid = np.linspace(tidy_data.lon.min(), tidy_data.lon.max(), 100)
lat_grid = np.linspace(tidy_data.lat.min(), tidy_data.lat.max(), 100)
lon_grid, lat_grid = np.meshgrid(lon_grid, lat_grid)
sst_grid = griddata((tidy_data.lon, tidy_data.lat), tidy_data.sst, (lon_grid, lat_grid), method='linear')
plt.contourf(lon_grid, lat_grid, sst_grid, cmap='jet', levels=20)


# plt.scatter(tidy_data.lon, tidy_data.lat, c=tidy_data.sst, cmap='jet', alpha=0.7, s=150, vmin=20, vmax=35)
plt.colorbar(label='Temperature')

x_tick=np.arange(130, 290, 10)
x_label=[f'{x}째E' if x <= 180 else f'{360 - x}째W' for x in x_tick]

y_tick = np.arange(-20, 25, 5)
y_label = [f'{np.abs(x)}째S' if x < 0 else f'{np.abs(x)}째N' for x in y_tick]

box = plt.Rectangle((190, -5), 50, 10, linewidth=2, edgecolor='RED', facecolor='none')
plt.gca().add_patch(box)

plt.axhline(y=0)

plt.xticks(ticks=x_tick, labels=x_label)
plt.yticks(ticks=y_tick, labels=y_label)

plt.xlim(130, 280)
plt.ylim(-20, 20)
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title(f'Equatorian Distribution of SST for {current_month.month_name()}-{current_month.year}')  # Title of the plot
plt.show()  # Display the plot








# result_list = extract_datasets('data/aqua_modis/elnino/2015/')


# dataset = result_list


# df = create_dataframe('esno_2015', *dataset)


df = pd.read_csv('data/csv_ready/elnino_2015.csv', index_col=0)





for index, data in df.groupby(df.index):
    print(index, data.sst.mean())


for index, data in df.groupby(df.index):
    print(index, data.sst.median())





df_enso_region = df[(df.lat >= -5) & (df.lat <= 5) | (df.lon >= -170) & (df.lon <= -120)]


df_enso_region.index = pd.to_datetime(df_enso_region.index)


df_enso_region


for date, data in df_enso_region.groupby(df_enso_region.index):
    print(date, f'Mean: {data.sst.mean().round(2)}, Median: {data.sst.median()}')





df_enso = df[(df.lat >= -5) & (df.lat <= 5) & (df.lon >= 190) & (df.lon <= 240)]


df.index = pd.to_datetime(df.index)


df.sst = df.sst.apply(lambda x: None if x > 33 else x)


df = df.dropna()


plot_equatorial_pacific(df, cond_name='El Nino')








# lanina = extract_datasets('data/aqua_modis/lanina/2010/')


# lanina = create_dataframe('lanina_2010', *lanina)


lanina = csv_from_aqua_modis_dataset('data/aqua_modis/lanina/2010/', 'lanina_2010')


lanina.sst = lanina.sst.apply(lambda x: None if x > 33 else x)


lanina = lanina.dropna()


plot_equatorial_pacific(lanina, cond_name='La Nina')





df = pd.read_csv('data/aqua_modis/elnino/1997/AVHRR_SST_M_1998-01-01_rgb_720x360.CSV', header=None)
lat = np.arange(-90, 90, 0.5)
lon = np.arange(-180, 180, 0.5)


df.columns = lon


df


df['lat'] = lat


df = df.melt(id_vars='lat', var_name='lon', value_name='sst')


df = df[(df.lat >= -20) & (df.lat <= 20)]


df = df[(df.lon >= -180) & (df.lon <= -80) | (df.lon >= 130) & (df.lon <= 180)]


df = df.reset_index(drop=True)


df.sst = df.sst.apply(lambda x: None if x > 100 else x)


df = df.dropna()


df['lon'] = np.where(df['lon'] < 0, df['lon'] + 360, df['lon'])





date = pd.to_datetime('1998-01')
df.index = pd.Index([date] * len(df))


plot_equatorial_pacific(df, cond_name='El Nino')


result = csv_from_avhrp_csv('data/aqua_modis/elnino/1997/', 'elnino_1997', '1997-10')


result = result.dropna()


plot_equatorial_pacific(result, 'El Nino')





tc_pacific = pd.read_csv('data/hurdat_pacific/hurdat2-nepac-1949-2023-042624.txt')


test = tc_pacific


# drop the index
test = test.reset_index()


# rename columns from 0 to len
test.columns = [i for i in range(len(test.columns))]


# drop columns 8 - end
test = test.drop(columns=[i for i in test.columns[8:]])


# name the columns
test.columns = ['date', 'time', 'consecutive_count', 'type_of_depression', 'lat', 'lon','max_wind_kn', 'min_pressure_mBar']


# changing time col to str 
test.time = test.time.astype(str)


# trimming time col values
test.time = test.time.apply(lambda x: x.strip())


test


def clean_ne_td_data(df):
    # reset index
    df = df.reset_index()
    # rename columns from 0 to len
    df.columns = [i for i in range(len(df.columns))]
    # drop columns 8 - end
    df = df.drop(columns=[i for i in df.columns[8:]])
    # name the columns
    df.columns = ['date', 'time', 'consecutive_count', 'type_of_depression', 'lat', 'lon', 'max_wind_kn',
                  'min_pressure_mBar']

    # modify the time
    df.time = df.time.astype(str)
    df.time = df.time.apply(lambda x: x.strip())

    # adding columns, removing non-observation entries
    df = export_name_basin_count(df)

    # continue to modify time
    df.time = df.time.apply(lambda x: x[0:2] + ':' + x[2:] + ':00')
    df.time = pd.to_timedelta(df.time)

    # strip the column, drop non-valid values and convert to int
    df.consecutive_count = df.consecutive_count.apply(lambda x: x.strip())
    df = df.drop(df.index[~df.consecutive_count.str.isdigit()])
    df = df.drop(df.index[~df.date.str.isdigit()])
    df.consecutive_count = df.consecutive_count.astype(int)

    # modify date, concatenate with time and set as index
    df.date = pd.to_datetime(df.date)
    df.date = df.date + df.time
    df.index = pd.Index(df.date)

    # drop the time and date
    df = df.drop(columns=['time', 'date'])
    
    # set the lat and lon
    df.lat = df.lat.apply(convert_lat_lon_to_number)
    df.lon = df.lon.apply(convert_lat_lon_to_number)

    # rearrange cols
    df = df[['basin', 'name', 'consecutive_count', 'type_of_depression', 'lat', 'lon', 'max_wind_kn',
       'min_pressure_mBar']]
    
    return df

def convert_lat_lon_to_number(value):
    return float(value[:-1]) if (value[-1] == 'N' or value[-1] == 'E') else float(value[:-1]) * -1 + 360

def export_name_basin_count(df):
    # adding additional cols for the type, name
    df['basin'] = pd.Series()
    df['name'] = pd.Series()
    
    # getting the rows with names
    names = df[~df.time.str.isdigit()]

    for index in names.index:
        basin = str(names.loc[index, 'date'])[:2]
        name = names.loc[index, 'time']
        count = names.loc[index, 'consecutive_count']

        start_index = index + 1
        while df.loc[start_index, 'time'].isdigit():
            df.loc[start_index, 'basin'] = basin
            df.loc[start_index, 'name'] = name
            df.loc[start_index, 'consecutive_count'] = count
            start_index += 1
            if start_index > df.index[-1]:
                break
        df = df.drop(index)
    return df


test = export_name_basin_count(test)


test


# strip the column and convert to int
test.consecutive_count = test.consecutive_count.apply(lambda x: x.strip())


# for this dropping non-valid rows
test = test.drop(test.index[~test.consecutive_count.str.isdigit()])
test = test.drop(test.index[~test.date.str.isdigit()])


# setting the type of the col
test.consecutive_count = test.consecutive_count.astype(int)


test.type_of_depression = test.type_of_depression.apply(lambda x: str(x).strip())


test.date = pd.to_datetime(test.date)


test.time = test.time.apply(lambda x: x[0:2] + ':' + x[2:] + ':00')


test.time = pd.to_timedelta(test.time)


test.date = test.date + test.time


test.index = pd.Index(test.date)


test = test.drop(columns=['time', 'date'])


def convert_lat_lon_to_number(value):
    return float(value[:-1]) if (value[-1] == 'N' or value[-1] == 'E') else float(value[:-1]) * -1 + 360


test.lat = test.lat.apply(convert_lat_lon_to_number)
test.lon = test.lon.apply(convert_lat_lon_to_number)





test.min_pressure_mBar.min()


test2 = tc_pacific[-80:]


test_2 = pd.read_csv('data/hurdat_pacific/hurdat2-nepac-1949-2023-042624.txt')


# tt = clean_ne_td_data(test_2)


# tt.to_csv('data/csv_ready/ne_pacific_td.csv')


tt = pd.read_csv('data/csv_ready/ne_pacific_td.csv', index_col=0)


tt = tt.drop(tt.index[tt.min_pressure_mBar == -999])


tt.index = pd.to_datetime(tt.index)


tt[tt.index.year == 1959][-50:]





header_col_space = [5, 4, 3, 4, 4, 1, 20, 8]
extract_names = pd.read_fwf('data/jma_data/bst_all.txt', widths=header_col_space, header=None)


names = extract_names[extract_names[0] == 66666]


names


data_col_space = [8, 4, 2, 4, 5, 5, 13] # manually adjustd
full_data = pd.read_fwf('data/jma_data/bst_all.txt', widths=data_col_space, header=None, dtype='str')


full_data[-50:]


def clean_jma_data(data_path='data/jma_data/bst_all.txt'):
    # read the data and extract the header rows 
    header_col_space = [5, 4, 3, 4, 4, 1, 20, 8]
    extract_names = pd.read_fwf(data_path, widths=header_col_space, header=None)
    names = extract_names[extract_names[0] == 66666]

    # read the data and extract the header rows. the column space is manually adjusted
    data_col_space = [8, 4, 2, 4, 5, 5, 13]
    full_data = pd.read_fwf(data_path, widths=data_col_space, header=None, dtype='str')

    # create a column to contain the names
    full_data['name'] = pd.Series()

    for index in names.index:
        name = names.loc[index, 7]

        start_index = index + 1
        while True:
            full_data.loc[start_index, 'name'] = name
            start_index += 1
            if start_index > full_data.index[-1]:
                break
            if start_index in names.index:
                break
        full_data = full_data.drop(index)

    col_names = ['date', 'indicator', 'category', 'lat', 'lon', 'min_pressure_mBar', 'max_wind_kn', 'name']
    full_data.columns = col_names
    full_data = full_data.drop(columns='indicator')

    full_data = cleaning_jma_columns(full_data)
    full_data = modify_jma_date(full_data)

    save_data(full_data, 'jma_td.csv')

    return full_data


data = clean_jma_data()


col_name = ['date', 'indicator', 'category', 	'lat',	'lon',	'min_pressure_mBar','max_wind_kn'	, 'name']


data.columns = col_name


df = data


df = df.drop(columns='indicator')


df.max_wind_kn = df.max_wind_kn.apply(lambda x: '000' if pd.isna(x) else x)


df.max_wind_kn = df.max_wind_kn.astype('int')


df.min_pressure_mBar = df.min_pressure_mBar.astype(int)


df.lat = df.lat.astype(int).apply(lambda x: x / 10)


df.lon = df.lon.astype(int).apply(lambda x: x / 10)


df.date = df.date.apply(jma_date_time)


df.index = pd.Index(df.date)


df = df.drop(columns='date')


df.to_csv('data/csv_ready/jma_td.csv')


def save_data(data, filename):
    try:
        data.to_csv(f'data/csv_ready/{filename}.csv')
    except OSError:
        print('Error saving file')

def modify_jma_date(df):
    df.date = df.date.apply(jma_date_time)
    df.index = pd.Index(df.date)
    df = df.drop(columns='date')

    return df


def cleaning_jma_columns(df):
    # wind column
    df.max_wind_kn = df.max_wind_kn.apply(lambda x: '000' if pd.isna(x) else x)
    df.max_wind_kn = df.max_wind_kn.astype('int')

    # pressure column
    df.min_pressure_mBar = df.min_pressure_mBar.astype(int)

    # latitude
    df.lat = df.lat.astype(int).apply(lambda x: x / 10)

    # longitude
    df.lon = df.lon.astype(int).apply(lambda x: x / 10)

    return df


def jma_date_time(value):
    """ will be applied to all values in the column """

    date = value[:-2]
    time = value[-2:]

    time = pd.Timedelta(hours=int(time))

    date = f'{date[:2]}-{date[2:4]}-{date[-2:]}'

    if int(date[:2]) < 51:
        date = '20' + date
    else:
        date = '19' + date

    date = pd.to_datetime(date)

    date += time

    return date


df_final = clean_jma_data()


df_final.to_csv('data/csv_ready/jma_td.csv')


jma = pd.read_csv('data/csv_ready/jma_td.csv', index_col=0)


jma.index = pd.to_datetime(jma.index)





jma[(jma.name == 'BOLAVE') & (jma.index.year == 2023)]


jma[jma.index.year == 2023].groupby(['name']).value_counts()


gdf = pd.read_csv('data/csv_ready/gdf_csv.csv')


gdf = gdf.drop(columns=['id', 'source', 'level', 'parent_id', 'sibling_id', 'area', 'geometry'])


gdf = gdf.drop(columns='geometry')


gdf.lon = gdf.lon.apply(lambda x: x + 360 if x < 0 else x)


gdf = gdf[gdf.lat <= 45]


gdf = gdf[gdf.lat >= -20]


gdf = gdf[gdf.lon >= 100]


gdf = gdf[gdf.lon <= 290]


plt.figure(figsize=(30, 10))
plt.scatter(gdf.lon, gdf.lat, s=0.5)

plt.show()


gdf.to_csv('data/csv_ready/gdf_pacific.csv', index=False)


pd.read_csv('data/csv_ready/gdf_pacific.csv')


jma_2023 = jma[jma.index.year == 2023]


jma_2023[jma_2023.name == 'KHANU']


plt.figure(figsize=(30, 10))
plt.scatter(gdf.lon, gdf.lat, s=0.5)

for td_name, values in jma_2023.groupby('name'):
    plt.plot(values['lon'], values['lat'], label=td_name)    

plt.show()


from sst_utils import read_files, dataset_to_csv
from plotting import plot_equatorial_pacific


result = read_files(directory_name='data/aqua_modis/neutral/', ext='nc')
neutral_csv = dataset_to_csv('neutral_2012', result)


neutral = pd.read_csv('data/csv_ready/neutral_2012.csv', index_col=0)


neutral.index = pd.to_datetime(neutral.index)


neutral.index = pd.to_datetime(neutral.index)neutral.index = pd.to_datetime(neutral.index)


elnino = pd.read_csv('data/csv_ready/elnino_2015.csv', index_col=0)
elnino.index = pd.to_datetime(elnino.index)


elnino


# result = read_files(directory_name='data/aqua_modis/lanina/2010/', ext='nc')
# lanina = dataset_to_csv('lanina_2010', result)


table = pd.read_table('data/year_period.txt', header=None)


cols = ['date', 'djf','jfm','fma','mam','amj','mjj','jja','jas','aso','son','ond','ndj']
table.columns = cols


table['enso'] = pd.Series()


table.loc[0, 'enso'] = -1


def apply_enso(df):
    for i in range(len(df)):
        if i == 0:
            continue
        if df.ndj[i - 1] > 0.5:
            df.loc[i, 'enso'] = 1
        elif df.ndj[i - 1] < -0.5:
            df.loc[i, 'enso'] = -1
        else:
            df.loc[i, 'enso'] = 0
    return df
table = apply_enso(table)


table.enso.value_counts()


cols = ['date', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 'enso']


table.columns = cols


table = table.melt(id_vars=['date', 'enso'], var_name='month', value_name='sst_anomaly').sort_values(['date', 'month'])


def concat_year_month(df):
    df.date = df.date.astype(str)
    df.month = df.month.astype(str)
    for i in range(len(df)):
        month = df.loc[i, 'month']
        year = df.loc[i, 'date']
        if len(month) == 1:
            month = '0' + month
        df.loc[i, 'date'] = f'{year}-{month}'

    df.date = pd.to_datetime(df.date)
    df.index = pd.Index(df.date)
    df = df.drop(columns=['date','month'])
    return df
table = concat_year_month(table)


table.to_csv('data/csv_ready/oni_table.csv')









